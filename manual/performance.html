<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="ru" xml:lang="ru">
<head>
<!-- 2017-01-10 Вт. 17:42 -->
<meta  http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta  name="viewport" content="width=device-width, initial-scale=1" />
<title>Материалы по оценке производительности вычислительных систем</title>
<meta  name="generator" content="Org-mode" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  pre.src-sh:before    { content: 'sh'; }
  pre.src-bash:before  { content: 'sh'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-R:before     { content: 'R'; }
  pre.src-perl:before  { content: 'Perl'; }
  pre.src-java:before  { content: 'Java'; }
  pre.src-sql:before   { content: 'SQL'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  /*]]>*/-->
</style>
<link rel="stylesheet" type="text/css" href="../org-html-themes/styles/readtheorg/css/htmlize.css"/>
<link rel="stylesheet" type="text/css" href="../org-html-themes/styles/readtheorg/css/readtheorg.css"/>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js"></script>
<script type="text/javascript" src="../org-html-themes/styles/lib/js/jquery.stickytableheaders.min.js"></script>
<script type="text/javascript" src="../org-html-themes/styles/readtheorg/js/readtheorg.js"></script>
<style type="text/css">.org-src-name{ text-align: right; }</style>
<style type="text/css">.outline-2{ margin-top: 60px; }</style>

<style type="text/css">
  pre.src:before {
    top: -5px;
  }
</style>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2013 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">Материалы по оценке производительности вычислительных систем</h1>
<div id="table-of-contents">
<h2>&#1057;&#1086;&#1076;&#1077;&#1088;&#1078;&#1072;&#1085;&#1080;&#1077;</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#orgheadline2">1. <span class="todo nilTODO">TODO</span> Сформировать набор тестов</a>
<ul>
<li><a href="#orgheadline1">1.1. Бенчмарки и тесты</a></li>
</ul>
</li>
<li><a href="#orgheadline4">2. <span class="todo nilTODO">TODO</span> развертывание кластера с помощью xCAT</a>
<ul>
<li><a href="#orgheadline3">2.1. xcat 2 Extreme Cloud Administration Toolkit</a></li>
</ul>
</li>
<li><a href="#orgheadline5">3. Обзорные работы, публикуемые организациями для своих вычислительных систем</a></li>
<li><a href="#orgheadline18">4. Ключевые слова</a>
<ul>
<li><a href="#linpack">4.1. LINPACK</a></li>
<li><a href="#p100">4.2. NVIDIA P100 Pascal</a>
<ul>
<li><a href="#nvlink">4.2.1. NVLink</a>
<ul>
<li><a href="#orgheadline7">4.2.1.1. NVLink 1.0</a></li>
<li><a href="#orgheadline8">4.2.1.2. NVLink 2.0 (2017)</a></li>
</ul>
</li>
<li><a href="#orgheadline10">4.2.2. Tesla P100 for PCIe-Based Servers</a></li>
</ul>
</li>
<li><a href="#power8">4.3. POWER 8</a>
<ul>
<li><a href="#orgheadline12">4.3.1. Servers</a></li>
<li><a href="#orgheadline13">4.3.2. Cell, PowerXCell, PowerPC Element</a></li>
<li><a href="#power">4.3.3. POWER</a></li>
</ul>
</li>
<li><a href="#orgheadline17">4.4. S822LC</a>
<ul>
<li><a href="#implementing-s822lc">4.4.1. Implementing an IBM High-Performance Computing Solution on IBM Power System S822LC</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgheadline19">5. Литература</a></li>
</ul>
</div>
</div>
<div id="bibliography">
<h2>References</h2>

<table>

<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="gorbunov2013komplexnaya">1</a>]
</td>
<td class="bibtexitem">
В.С. Горбунов and Л.К. Эйсымонт.
 Комплексная методика тестирования
  производительности суперкомпьютеров.
 <em>Выч. мет</em>, 14(4):115-121, 2013.
[&nbsp;<a href="cluster_bib.html#gorbunov2013komplexnaya">bib</a>&nbsp;| 
<a href="http://www.mathnet.ru/php/archive.phtml?wshow=paper&jrnid=vmp&paperid=160&option_lang=rus">http</a>&nbsp;]
<blockquote><font size="-1">
Рассматривается рейтинговое и профессиональное
                  (комплексное и углубленное) оценочное
                  тестирование. Оба подхода представляют интерес. Это
                  демонстрируется сначала на примере анализа динамики
                  результатов рейтингового тестирования последних лет,
                  позволившего выявить устойчивую тенденцию развития
                  суперкомпьютеров в сторону повышения эффективности
                  решения задач с интенсивной нерегулярной работой с
                  памятью. Далее рассматриваются схемы применения
                  профессионального оценочного тестирования при оценке
                  и разработке суперкомпьютеров, а также пример такой
                  методики, применяемой авторами. Эта методика
                  особенна своей многоуровневостью и разноплановостью
                  на уровнях, обеспечиваемой тестами разной
                  направленности и специальным образом изменяемой
                  нагрузкой на оборудование. Сообщается о вариантах
                  практического применения методики и использования ее
                  результатов при оптимизации программ и создании
                  перспективного системного программного
                  обеспечения. Статья рекомендована к публикации
                  Программным комитетом Международной научной
                  конференции “Научный сервис в сети Интернет: все
                  грани параллелизма”
                  (http://agora.guru.ru/abrau2013).
</font></blockquote>
<p><blockquote><font size="-1">
Keywords: pейтинговое тестирование; комплексное тестирование;
                  задачи с интенсивной нерегулярной работой с памятью;
                  многоуровневое оценочное тестирование;
                  пространственно-временная локализация обращений к
                  памяти; оптимизация задержек обращений к памяти;
                  оптимизация толерантности к задержкам обращений к
                  памяти; системы поддержки выполнения программ;
                  программная эмуляция мультитредовых архитектур
</font></blockquote>

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="kulikov2016issledovanie">2</a>]
</td>
<td class="bibtexitem">
V.&nbsp;Kulikov.
 ИССЛЕДОВАНИЕ ВОЗМОЖНОСТЕЙ gpu В
  ВЫСОКОПРОИЗВОДИТЕЛЬНЫХ ВЫЧИСЛЕНИЯХ.
 <em>В мире научных открытий</em>, (12.2):639-650,
  2015.
[&nbsp;<a href="cluster_bib.html#kulikov2016issledovanie">bib</a>&nbsp;| 
<a href="https://istina.msu.ru/publications/article/17173674/">http</a>&nbsp;]
<blockquote><font size="-1">
Статья посвящена исследованию вопроса эффективности
                  в высокопроизводительном гибридном вычислительном
                  кластере на основе графических процессоров
                  (GPU). Автор обращает особое внимание на
                  формирование программно-математического окружения
                  для проведения исследования в направлении
                  производительности кластера параллельных вычислений
                  с помощью теста Linpack, раскрывающего зависимость
                  производительности кластера от увеличения расчетной
                  нагрузки. Приведенные в статье результаты
                  исследования используются в построении входных
                  данных при подготовке моделей Ansys, Capvidia
                  FlowVision.
</font></blockquote>
<p><blockquote><font size="-1">
Keywords: ГРАФИЧЕСКИЙ ПРОЦЕССОР; КЛАСТЕР; ЭФФЕКТИВНАЯ
                  ПРОИЗВОДИТЕЛЬНОСТЬ; ВЫСОКОПРОИЗВОДИТЕЛЬНЫЕ
                  ВЫЧИСЛЕНИЯ; РАЗМЕРНОСТЬ ЗАДАЧИ; ГИБРИДНЫЙ УЗЕЛ;
                  ФЛОПС
</font></blockquote>

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="quintero2016implementing">3</a>]
</td>
<td class="bibtexitem">
D.&nbsp;Quintero, L.C.C. Huertas, T.&nbsp;Kamenoue, W.&nbsp;dos Santos&nbsp;Moschetta, M.F.
  de&nbsp;Oliveira, G.E. Pavlov, A.&nbsp;Pozdneev, and IBM Redbooks.
 <em>Implementing an IBM High-Performance Computing Solution on IBM
  Power System S822LC</em>.
 IBM Redbooks, 2016.
[&nbsp;<a href="cluster_bib.html#quintero2016implementing">bib</a>&nbsp;| 
<a href="http://www.redbooks.ibm.com/redbooks/pdfs/sg248280.pdf">.pdf</a>&nbsp;]

</td>
</tr>
</table>
</div>

<div id="outline-container-orgheadline2" class="outline-2">
<h2 id="orgheadline2"><span class="section-number-2">1</span> <span class="todo TODO">TODO</span> Сформировать набор тестов</h2>
<div class="outline-text-2" id="text-1">
<blockquote>
<p>
Необходимо найти и изучить литературу по оценке производительности
высокопроизводительных вычислительных систем. Основная цель -
сформировать набор тестов, который будет использоваться при вводе в
эксплуатацию нового вычислительного кластера. Особое внимание уделить
вопросам тестирования <b>GPU</b> (NVIDIA <b>P100</b>) и процессоров с архитектурой
<b>POWER8</b>. Также нужно поискать примеры обзорных работ, публикуемых
организациями для своих вычислительных систем.
</p>

<p>
Ключевые слова: обзорная статья, <b>S822LC</b>, <b>CUDA 8</b>, <b>Pascal</b>, <b>NVLink</b>,
<b>LINPACK</b>, <b>Green500</b>, <b>Graph500</b>, плотнозаполненные матрицы, тесты
для новой архитектуры.
</p>
</blockquote>
</div>

<div id="outline-container-orgheadline1" class="outline-3">
<h3 id="orgheadline1"><span class="section-number-3">1.1</span> Бенчмарки и тесты</h3>
<div class="outline-text-3" id="text-1-1">
<ul class="org-ul">
<li><a href="https://www-01.ibm.com/common/ssi/cgi-bin/ssialias?htmlfid=POD03117USEN">буклет</a> IBM Power System S822LC for High Performance Computing
<ul class="org-ul">
<li>Kinetica “Filter by geographic area” queries on data set of 280
million simulated Tweets with 1 up to 80 concurrent query streams
each with 0 think time</li>
<li>LatticeQCD</li>
<li>SOAP3-dp</li>
<li>CPMD, a parallelized plane wave /pseudopotential implementation of
Density Functional Theory. A Hybrid version of CPMD (e.g. MPI +
OPENMP + GPU + streams)</li>
<li>High Performance Conjugate Gradients (HPCG) Benchmark <a href="http://www.hpcg-benchmark.org/">http://www.hpcg-benchmark.org/</a></li>
</ul></li>
</ul>


<ul class="org-ul">
<li>Комплексная методика тестирования производительности суперкомпьютеров [<a href="#gorbunov2013komplexnaya">1</a>]
<ul class="org-ul">
<li>Класс DIS-задач представляют тест
<ul class="org-ul">
<li>BFS (рейтинг Graph500);</li>
<li>3/4 тестов HPC Challenge Class 1 Awards
<ul class="org-ul">
<li>G-RandomAccess</li>
<li>G-FFT</li>
<li>EP-STREAM-Triad</li>
</ul></li>
</ul></li>
<li>СF-задачи
<ul class="org-ul">
<li>Linpack (рейтинг Top500)</li>
<li>G-HPL (рейтинг HPC Challenge Class 1 Awards)</li>
</ul></li>
<li>Многоуровневая методика оценочного тестирования
<ul class="org-ul">
<li>Первый уровень: оценка подсистемы памяти на тесте APEX-MAP, он
искусственно меняет пространственно-временн́ую локализацию адресов
обращений и определяет среднее количество тактов на
однообращение. По результатам строится APEX-поверхность зависимость
тактов процессора, затраченных на обращение к памяти, от временн́ой и
пространственной локализации.</li>
<li>Второй уровень: граничные тесты, соответствуют четырем предельным
значениям пространственно-временн́ой локализации: Linpack (хорошая
пространственная и временн́ая локализация), Random Access
(одновременно плохая пространственная и временн́ая), PTRANS и TRIAD
(плохая временн́ая, хорошая пространственная), FFT (хорошая
временн́ая, плохая пространственная локализация).</li>
<li>Третий уровень: специально подобранные тесты для детального
исследования оборудования, а именно процессорных функциональных
устройств выполнения арифметико-логических операций и операций с
памятью, внутренней и внешней сети вычислительных узлов, системы
ввода-вывода.</li>
<li>Четвертый уровень: общие и специальные базовые алгоритмы прикладных
программ: стандартные математические функции, векторные операции,
векторно-матричные операции (включая операции с разреженными
матрицами), матричные операции, операции с битовыми матрицами,
операции специальных преобразований при обработке сигналов, операции
целочисленной арифметики многократной точности.</li>
<li>Пятый уровень: ядра разных приложений, а именно научные расчеты
(линейная алгебра и аэро-гидродинамика); сжатие текстов и
изображений; защита информации; обработка текстов и изображений;
задачи оптимизации и поиска; задачи на высокорегулярных
структурах (сеточные методы и клеточные алгоритмы); задачи на
деревьях и графах; тесты разных моделей организации параллельных
программ.</li>
<li>Шестой уровень: тесты модельных приложений, а также операционной
системы и системы планирования прохождения заданий.</li>
</ul></li>
</ul></li>
</ul>
</div>
</div>
</div>

<div id="outline-container-orgheadline4" class="outline-2">
<h2 id="orgheadline4"><span class="section-number-2">2</span> <span class="todo TODO">TODO</span> развертывание кластера с помощью xCAT</h2>
<div class="outline-text-2" id="text-2">
<ul class="org-ul">
<li>см. также <a href="#implementing-s822lc">s822lc</a></li>
</ul>
</div>
<div id="outline-container-orgheadline3" class="outline-3">
<h3 id="orgheadline3"><span class="section-number-3">2.1</span> xcat 2 Extreme Cloud Administration Toolkit</h3>
<div class="outline-text-3" id="text-2-1">
<ul class="org-ul">
<li><a href="http://docplayer.net/3806990-Xcat-2-extreme-cloud-administration-toolkit.html">презентация</a>
<ul class="org-ul">
<li>xCAT Architecture &amp; Basic Functionality
<ul class="org-ul">
<li>"Nodes" are anything that require an IP address:
<ul class="org-ul">
<li>Servers, server IMM/BMCs</li>
<li>Switches, storage controllers, PDUs, etc.</li>
</ul></li>
<li>XCAT provides a choice – conventional state-full installs to disk,
state-less installs with no persistent media, and state-lite
installs with persistent media NFS mounted.</li>
<li>Statelite advantages over Stateless
<ul class="org-ul">
<li>Some files can be made persistent over reboot. However, you
still get the advantage of only having to manage a single image</li>
<li>Changes to hundreds of machines can take place instantly, and
automatically, by updating one main image. In most cases,
machines do not need to reboot for these changes to take
affect. (only for the NFSROOT-based solution)</li>
<li>Ease of administration by being able to lock down an
image. Parts of the image can be read-only - so no modifications
can be made without updating the central image</li>
<li>Files can be managed . You can set table values to allow
machines to sync from different places based on their
attributes: think overlays</li>
<li>Ideal for virtualization. Images can be smaller, use less disk
and memory, and can be easier to manage</li>
</ul></li>
<li>Statelite disadvantages compared to Stateless
<ul class="org-ul">
<li>Network traffic is not just at node boot time</li>
<li>NFS and scaling</li>
<li>While more flexible it does require more initial configuration
to work</li>
<li>Some limitations on target environments. Mostly a testing
problem</li>
</ul></li>
</ul></li>
<li>xCAT Commands
<ul class="org-ul">
<li>xCAT Commands used on the CLI interface can be divided in several groups
<ul class="org-ul">
<li>Database support
<ul class="org-ul">
<li>chtab, chdef, nodels, mkrrbc, mkrrnodes, modech, tabdump</li>
</ul></li>
<li>Hardware control
<ul class="org-ul">
<li>getmacs, rcons, renergy, rnetboot, reventlog</li>
</ul></li>
<li>Monitoring
<ul class="org-ul">
<li>monadd, monls, monstart, monstop</li>
</ul></li>
<li>Inventory
<ul class="org-ul">
<li>rinv, rvitals, sinv</li>
</ul></li>
<li>Parallel commands
<ul class="org-ul">
<li>pscp, psh, prsync, pping</li>
</ul></li>
<li>Deployment
<ul class="org-ul">
<li>copycds, genimage, liteimg</li>
</ul></li>
<li>CSM to xCAT migration Tools</li>
<li>Others
<ul class="org-ul">
<li>makenetworks, makehost, makedhcp</li>
</ul></li>
</ul></li>
<li>xCAT <a href="http://xcat-docs.readthedocs.io/en/latest/guides/admin-guides/references/man3/noderange.3.html">noderange</a></li>
</ul></li>
<li>Setting Up an xCAT Cluster
<ul class="org-ul">
<li>Prior to installing the Management Node, the administrator
should already have an idea of how the overall cluster will look
post-configured.
<ul class="org-ul">
<li>Internal Node naming scheme</li>
<li>Networks and IP addresses for every node on each net</li>
<li>Storage disk drive and filesystem layout</li>
<li>Linux version(s) to be used</li>
<li>Node or resource groups</li>
<li>Resource manager/scheduler</li>
<li>Cluster authentication method</li>
<li>License managers/services inside/outside the cluster</li>
</ul></li>
</ul></li>
</ul></li>
</ul>
</div>
</div>
</div>

<div id="outline-container-orgheadline5" class="outline-2">
<h2 id="orgheadline5"><span class="section-number-2">3</span> Обзорные работы, публикуемые организациями для своих вычислительных систем</h2>
<div class="outline-text-2" id="text-3">
<ul class="org-ul">
<li>Т-Платформы, Ломоносов, 2012 <a href="http://www.t-platforms.ru/images/pdfsolutions_RUS/%D0%A1%D1%83%D0%BF%D0%B5%D1%80%D0%BA%D0%BE%D0%BC%D0%BF%D1%8C%D1%8E%D1%82%D0%B5%D1%80%20%D0%9B%D0%BE%D0%BC%D0%BE%D0%BD%D0%BE%D1%81%D0%BE%D0%B2.pdf">link</a></li>
<li>Отчет о введении гибридного кластера ФМБФ в эксплуатацию 2010
<ul class="org-ul">
<li><a href="http://miptic.ru/about/cluster_report_2010.php">http://miptic.ru/about/cluster_report_2010.php</a></li>
</ul></li>
<li>В августе 2011 года компания «Интант» завершила работы по созданию
вычислительного кластера в Томском Государственном Университете.
<ul class="org-ul">
<li><a href="http://www.intant.ru/Events/News/7593.aspx">http://www.intant.ru/Events/News/7593.aspx</a></li>
</ul></li>
<li>Ввод в постоянную эксплуатацию нового вычислительного кластера СПбГПУ / 10.02.2012
<ul class="org-ul">
<li><a href="http://www.ict.edu.ru/news/tech/4628/">http://www.ict.edu.ru/news/tech/4628/</a></li>
</ul></li>
<li>Отчет о результатах тестирования и оценке эффективности
вычислительных кластеров на базе учебных компьютерных классов 2010
<ul class="org-ul">
<li><a href="http://ofap.ulstu.ru/attachments/55/download">http://ofap.ulstu.ru/attachments/55/download</a></li>
</ul></li>
<li>Проектирование вычислительного кластера для решения задач прикладной
механики деформируемого твердого тела, вычислительной гидро- и
газодинамики
<ul class="org-ul">
<li><a href="http://isicad.ru/ru/articles.php?article_num=18793">http://isicad.ru/ru/articles.php?article_num=18793</a></li>
<li>В рамках нагрузочного тестирования проведены тестовые расчеты,
призванные продемонстрировать быстродействие системы. Первый из
таких расчетов - расчет реактивного двигателя, модель которого
содержит 12 миллионов полиэдральных ячеек. Расчет выполнен
средствами ANSYS Fluent 17.0, используется k-epsilon модель
турбулентности. Кроме гидродинамических расчетов в программу
тестирования также вошли задачи механики. ANSYS Mechanical 17.0
показал отличные результаты при распараллеливании задач решаемых
прямым и итерационным методом. Так, при расчете линейной
статической прочности задней оси трактора итеративным решателем
PCG, система показала ускорение на 30-40% при решении 12.3
миллионов уравнений. Для прямого решателя Sparce результаты
расчета с использованием Tesla K80 оказались еще более
внушительными: нелинейная задача ползучести для BGA-чипа,
содержащая 6.0 миллионов уравнений, была решена в 2 раза быстрее с
использованием GPGPU.</li>
</ul></li>
<li>Отчет о работе ресурсного центра «Вычислительный центр СПбГУ», 2013
<ul class="org-ul">
<li><a href="http://www.cc.spbu.ru/sites/default/files/RCCC.14-0-548.pdf">http://www.cc.spbu.ru/sites/default/files/RCCC.14-0-548.pdf</a></li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline18" class="outline-2">
<h2 id="orgheadline18"><span class="section-number-2">4</span> Ключевые слова</h2>
<div class="outline-text-2" id="text-4">
</div><div id="outline-container-orgheadline6" class="outline-3">
<h3 id="linpack"><a id="orgheadline6"></a><a id="ID-7913bb34-5ecd-4445-bb30-eb8747a4128c"></a><span class="section-number-3">4.1</span> LINPACK</h3>
<div class="outline-text-3" id="text-linpack">
<ul class="org-ul">
<li><a href="https://www.nextplatform.com/2016/06/20/nvidia-rounds-pascal-tesla-accelerator-lineup/">Nvidia Rounds Out Pascal Tesla Accelerator Lineup</a>
<ul class="org-ul">
<li>Interestingly, because the M40 does not have DP math, it cannot
run the Linpack Fortran benchmark test, and it therefore cannot be
ranked on the Top 500 list of supercomputers.</li>
</ul></li>
<li>High Performance Linpack for GPUs (Using OpenCL, CUDA, CAL) <a href="https://github.com/davidrohr/hpl-gpu">https://github.com/davidrohr/hpl-gpu</a></li>
</ul>
</div>
</div>
<div id="outline-container-orgheadline11" class="outline-3">
<h3 id="p100"><a id="orgheadline11"></a><a id="ID-0a47544c-f8e5-440e-8812-8c9bee1a4344"></a><span class="section-number-3">4.2</span> NVIDIA P100 Pascal</h3>
<div class="outline-text-3" id="text-p100">
<ul class="org-ul">
<li>NVIDIA® Tesla® P100 GPU accelerators are the most advanced ever
built for the data center. They tap into the new NVIDIA Pascal™ GPU
architecture</li>

<li>INSIDE PASCAL
<ul class="org-ul">
<li><a href="http://on-demand.gputechconf.com/gtc/2016/presentation/s6176-mark-harris-lars-nyland.pdf">http://on-demand.gputechconf.com/gtc/2016/presentation/s6176-mark-harris-lars-nyland.pdf</a></li>
<li><a href="https://devblogs.nvidia.com/parallelforall/inside-pascal/">https://devblogs.nvidia.com/parallelforall/inside-pascal/</a>
<ul class="org-ul">
<li>Increasing Developer Productivity with Unified Memory
<ul class="org-ul">
<li>Unified Memory is an important feature of the CUDA programming
model that greatly simplifies programming and porting of
applications to GPUs by providing a single, unified virtual
address space for accessing all CPU and GPU memory in the
system.</li>
<li>The CUDA system software automatically migrates data allocated
in Unified Memory between GPU and CPU, so that it looks like
CPU memory to code running on the CPU, and like GPU memory to
code running on the GPU.</li>
</ul></li>
<li>Page faulting
<ul class="org-ul">
<li>First, page faulting means that the CUDA system software
doesn’t need to synchronize all managed memory allocations to
the GPU before each kernel launch. If a kernel running on the
GPU accesses a page that is not resident in its memory, it
faults, allowing the page to be automatically migrated to the
GPU memory on-demand. Alternatively, the page may be mapped
into the GPU address space for access over the PCIe or NVLink
interconnects (mapping on access can sometimes be faster than
migration). Note that Unified Memory is system-wide: GPUs (and
CPUs) can fault on and migrate memory pages either from CPU
memory or from the memory of other GPUs in the system.</li>
</ul></li>
</ul></li>
</ul></li>

<li>CUDA 8 Pascal
<ul class="org-ul">
<li>Allocate Beyond GPU Memory Size
<ul class="org-ul">
<li>Certain operating system modifications are required to enable
Unified Memory with the system allocator. NVIDIA is
collaborating with Red Hat and working within the Linux
community to enable this powerful functionality.</li>
</ul></li>
<li>Finally, on supporting platforms, memory allocated with the
default OS allocator (e.g. ‘malloc’ or ‘new’) can be accessed from
both GPU code and CPU code using the same pointer (see code
example below).
<ul class="org-ul">
<li>Moreover, GP100’s large virtual address space and page faulting
capability enable applications to access the entire system
virtual memory.</li>
</ul></li>
<li>With the new page fault mechanism, global data coherency is
guaranteed with Unified Memory. This means that with GP100, the
CPUs and GPUs can access Unified Memory allocations
simultaneously.
<ul class="org-ul">
<li>Note, as with any parallel application, developers need to
ensure correct synchronization to avoid data hazards between
processors.</li>
</ul></li>
<li>Expanding on the benefits of CUDA 6 Unified Memory, Pascal GP100
adds features to further simplify programming and sharing of
memory between CPU and GPU, and allowing easier porting of CPU
parallel compute applications to use GPUs for tremendous
speedups. Two main hardware features enable these improvements:
support for large address spaces and page faulting capability.</li>
<li>Therefore, GP100 Unified Memory allows programs to access the full
address spaces of all CPUs and GPUs in the system as a single
virtual address space, unlimited by the physical memory size of
any one processor.</li>
</ul></li>
<li>CUDA 6+ Kepler Maxwell
<ul class="org-ul">
<li>CUDA 6 introduced Unified Memory, which creates a pool of managed
memory that is shared between the CPU and GPU, bridging the
CPU-GPU divide. Managed memory is accessible to both the CPU and
GPU using a single pointer.</li>
<li>CUDA 6 Unified Memory was limited by the features of the Kepler
and Maxwell GPU architectures: all managed memory touched by the
CPU had to be <b>synchronized</b> with the GPU before any kernel launch;
the CPU and GPU could not simultaneously access a managed memory
allocation; and the Unified Memory address space was limited to
the size of the GPU physical memory.</li>
</ul></li>
</ul>
</div>
<div id="outline-container-orgheadline9" class="outline-4">
<h4 id="nvlink"><a id="orgheadline9"></a><a id="ID-438f5422-92d2-44dc-862b-bdec2d3b3095"></a><span class="section-number-4">4.2.1</span> NVLink</h4>
<div class="outline-text-4" id="text-nvlink">
<ul class="org-ul">
<li>Tesla P100 for NVLink-enabled Servers</li>
<li>Напомним, одной из особенностей стал интерфейс связи между
несколькими графическими NVIDIA. NVLink распределяет нагрузку между
GPU, увеличивая пропускную способность в 5 раз. NVIDIA NVLink
позволяет связать вместе до восьми карт Tesla P100. Двунаправленный
интерфейс NVIDIA NVLink обладает скоростью 160 ГБ/с.  Как отмечает
NVIDIA, Tesla P100 — первый ускоритель со скоростью вычислений
двойной и одинарной точности в 5 и 10 терафлопс соответственно.</li>
<li>NVLink — высокопроизводительная компьютерная шина, использующая
соединение точка-точка, дифференциальные сигналы со встроенным
синхросигналом и каналы, называемые «блоки», в каждом по 8 пар со
скоростью 20 Гбит/с. Таким образом каждый блок предоставляет
возможность передачи примерно 20 гигабайт в секунду.</li>
<li>Предполагается, что NVLink будет использовать мезонинный
разъем. Программная модель интерфейса NVLink схожа с PCI
Express. По интерфейсу NVLink несколько GPU будут связываться
друг с другом, а в дальнейшем планируется использовать его для связи
GPU и центрального процессора (возможно, с архитектурой IBM
<a href="#power8">POWER</a>), и добавление в интерфейс протоколов
кеш-когерентности.</li>
<li>The NVLink technology on the POWER platform provides coherency among
the multiple memory hierarchies in the CPUs and GPUs.</li>
<li>But most of this power will be wasted unless GPUs learn to exchange
data with CPUs at faster speeds. For years, Nvidia has been working
on a technology that could do just that, and it looks like NVLink is
finally ready for action.</li>
<li>Server-side GPU acceleration is a hot topic: earlier this year
Nvidia launched Tesla P100, calling it “the most advanced data
center GPU ever built’. Intel is pushing ahead with its Xeon Phi,
claiming it is more than a match for Nvidia’s hardware, and AMD is
steadily improving its FirePro range.</li>
</ul>
</div>
<div id="outline-container-orgheadline7" class="outline-5">
<h5 id="orgheadline7"><span class="section-number-5">4.2.1.1</span> NVLink 1.0</h5>
<div class="outline-text-5" id="text-4-2-1-1">
<ul class="org-ul">
<li>Пропускная способность интерфейса используемого в GPU NVIDIA Pascal GP100 (2016 год):
<ul class="org-ul">
<li>20 Гбит/с на контакт;</li>
<li>40 Гбайт/с на один порт;</li>
<li>160 Гбайт/с (4 × 40 Гбайт/с) на один GPU.</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgheadline8" class="outline-5">
<h5 id="orgheadline8"><span class="section-number-5">4.2.1.2</span> NVLink 2.0 (2017)</h5>
</div>
</div>
<div id="outline-container-orgheadline10" class="outline-4">
<h4 id="orgheadline10"><span class="section-number-4">4.2.2</span> Tesla P100 for PCIe-Based Servers</h4>
</div>
</div>
<div id="outline-container-orgheadline15" class="outline-3">
<h3 id="power8"><a id="orgheadline15"></a><a id="ID-858126b4-6dfd-487c-8840-5859d84b3fe1"></a><span class="section-number-3">4.3</span> POWER 8</h3>
<div class="outline-text-3" id="text-power8">
<ul class="org-ul">
<li>Представлен в 2013 году, изготовлен по 22 нм SOI. 6 или 12 ядер на
чип, тактовая частота от 2.5 до 5 ГГц, каждое ядро исполняет
одновременно до 8 потоков. процессор имеет общий кэш L3 размером 48
МБ(6-ядерные модели) или 96 МБ (12-ядерные модели). В процессор
встроены высокопроизводительные контроллеры памяти (DDR3/DDR4) и
системных каналов ввода-вывода (CAPI port на основе PCI
Express3.0 в том числе, для подключения ASIC, FPGA,
GPU. Питанием процессора управляет встроенный микроконтроллер на
базе PowerPC 405 с 512 килобайтами SRAM памяти, настраивая 1764
встроенных регуляторов напряжения. Векторно-скалярное устройство
процессора,для работы с числами c плавающей запятой выдает до 8
результатов с плавающей запятой двойной точности, что обеспечивает
пиковую производительность 384 GFLOPS на процессор. Для многих видов
нагрузок процессор POWER8 показывает прирост производительности в
2-3 раза по сравнению с предыдущим процессором POWER7.</li>
<li>IBM has launched a version of the Power8 processor that features
Nvidia’s NVLink, a high-performance interconnect technology that
sits between GPU and CPU.
<ul class="org-ul">
<li>The chip has been released as part of a new Power System server, the
S822LC for High Performance Computing.</li>
<li>IBM has also launched two servers without NVLink: the Power System
S821LC and the Power System S822LC for Big Data.</li>
</ul></li>
</ul>
</div>
<div id="outline-container-orgheadline12" class="outline-4">
<h4 id="orgheadline12"><span class="section-number-4">4.3.1</span> Servers</h4>
<div class="outline-text-4" id="text-4-3-1">
<ul class="org-ul">
<li><a href="https://www.microway.com/product/openpower-gpu-server-nvidia-tesla-p100-nvlink-gpus/">OpenPOWER GPU Server with NVIDIA Tesla P100 NVLink GPUs</a></li>
<li>IBM, NVIDIA и Wistron <a href="https://servernews.ru/931493">разработали</a> новый HPC-сервер на базе POWER8 и Tesla P100
<ul class="org-ul">
<li>Для того чтобы получить все преимущества от NVIDIA Tesla P100 с
шиной NVLink, программистам придётся переделать свои программы под
IBM POWER8. IBM и NVIDIA намерены создать сеть лабораторий, чтобы
помочь разработчикам приложений портировать свои программы на новые
высокопроизводительные вычислительные платформы. Эти лаборатории
будут очень важны не только для IBM и NVIDIA, но и для будущего
высокопроизводительных систем в целом. Гетерогенные суперкомпьютеры
могут предложить очень высокую производительность, но для того,
чтобы использовать их в полной мере, необходимы новые методы
программирования.</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgheadline13" class="outline-4">
<h4 id="orgheadline13"><span class="section-number-4">4.3.2</span> Cell, PowerXCell, PowerPC Element</h4>
<div class="outline-text-4" id="text-4-3-2">
<p>
Cell совмещает ядро общего назначения архитектуры <a href="#power">POWER</a> с
сопроцессорами, которые значительно ускоряют обработку мультимедиа
и векторных вычислений.
</p>

<p>
<a href="https://parallel.ru/computers/cell.html">parallel: Процессор CELL</a>
</p>
</div>
</div>

<div id="outline-container-orgheadline14" class="outline-4">
<h4 id="power"><a id="orgheadline14"></a><a id="ID-00567ea2-be80-4d73-8b3d-bcc5557f52e9"></a><span class="section-number-4">4.3.3</span> POWER</h4>
<div class="outline-text-4" id="text-power">
<ul class="org-ul">
<li>POWER — микропроцессорная архитектура с ограниченным набором команд
(RISC), разработанная и развиваемая компанией IBM. Название позже
было расшифровано как Performance Optimization With Enhanced RISC
(оптимизация производительности на базе расширенной архитектуры
RISC). Этим словом также называется серия микропроцессоров,
использующая указанный набор команд. Они применяются в качестве
центрального процессора во многих микрокомпьютерах, встраиваемых
системах, рабочих станциях, мейнфреймах и суперкомпьютерах.</li>
<li>Он содержал тридцать два 32-разрядных целочисленных регистра и ещё
тридцать два 64-разрядных регистра с плавающей точкой, каждый в
своём разделе. Кроме того, имелось несколько регистров для
внутренних нужд внутри блока ветвления, в частности, счётчик адреса.</li>
<li>Тогда как 801 был простым устройством, чрезмерное количество
дополнений превратили его в сложный процессор, гораздо сложнее
большинства конкурирующих RISC-изделий. Например, набор команд POWER
(и PowerPC) включает более 100 опкодов переменной длины, многие из
которых являются модификациями друг друга. Для сравнения:
архитектура ARM располагает только 34 инструкциями.</li>
<li>В конструкцию заложено и одно необычное свойство: виртуальное
адресное пространство. Все адреса во время работы конвертируются в
52-битное представление, таким образом получается, что каждая
программа обладает плоским 32-битным пространством адресов, но при
этом каждая может занимать эти блоки произвольно.</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-orgheadline17" class="outline-3">
<h3 id="orgheadline17"><span class="section-number-3">4.4</span> S822LC</h3>
<div class="outline-text-3" id="text-4-4">
<ul class="org-ul">
<li>Server-side GPU acceleration
<ul class="org-ul">
<li>But most of this power will be wasted unless GPUs learn to
exchange data with CPUs at faster speeds. For years, Nvidia has
been working on a technology that could do just that, and it looks
like NVLink is finally ready for action.</li>
</ul></li>
<li>буклет IBM Power System S822LC for High Performance Computing
<ul class="org-ul">
<li><a href="https://www-01.ibm.com/common/ssi/cgi-bin/ssialias?htmlfid=POD03117USEN">https://www-01.ibm.com/common/ssi/cgi-bin/ssialias?htmlfid=POD03117USEN</a>
<ul class="org-ul">
<li><a href="http://www-01.ibm.com/common/ssi/cgi-bin/ssialias?subtype=BR&amp;infotype=PM&amp;appname=STGE_PO_PO_USEN&amp;htmlfid=POB03046USEN">IBM Power Systems Facts and Features</a></li>
</ul></li>
<li>The combination of NVLink and NVIDIA Tesla P100 GPUs delivers
unprecedented performance across multiple workloads compared to
x86 with Tesla K80 GPUs:
<ul class="org-ul">
<li>2.5 times more queries per hour running Kinetica “Filter by
geographic area” queries</li>
<li>1.9 times more GFLOPS based on running LatticeQCD 32 times more
“Base Pairs Aligned” per Second running SOAP3-dp with 2 instances
per device.</li>
<li>2.3 times better performance (57 percent reduction in execution
time) running CPMD</li>
<li>1.7 times better performance running the High Performance
Conjugate Gradients (HPCG) Benchmark.</li>
</ul></li>
</ul></li>
<li>Operating systems
<ul class="org-ul">
<li>Linux on POWER
<ul class="org-ul">
<li>Ubuntu 16.04.1 LTS ppc64el on S822LC</li>
</ul></li>
</ul></li>
</ul>
</div>

<div id="outline-container-orgheadline16" class="outline-4">
<h4 id="implementing-s822lc"><a id="orgheadline16"></a><a id="ID-4e8e265c-1dd2-43a7-a1cb-a47f5e425b75"></a><span class="section-number-4">4.4.1</span> Implementing an IBM High-Performance Computing Solution on IBM Power System S822LC</h4>
<div class="outline-text-4" id="text-implementing-s822lc">
<ul class="org-ul">
<li>Selective Contents [<a href="#quintero2016implementing">3</a>]
<ul class="org-ul">
<li>Chapter 2. Reference architecture</li>
<li>Chapter 3. Hardware components</li>
<li>Chapter 4. Software stack
<ul class="org-ul">
<li>4.2 OPAL firmware</li>
<li>4.3 xCAT</li>
<li>4.4 RHEL server</li>
<li>4.5 NVIDIA CUDA Toolkit</li>
<li>4.6 Mellanox OFED for Linux</li>
<li>4.7 IBM XL compilers, GCC, and Advance Toolchain</li>
<li>4.8 IBM Parallel Environment</li>
<li>4.9 IBM Engineering and Scientific Subroutine Library and Parallel ESSL</li>
<li>4.10 IBM Spectrum Scale (formerly IBM GPFS)</li>
<li>4.11 IBM Spectrum LSF (formerly IBM Platform LSF)</li>
</ul></li>
<li>Chapter 5. Software deployment
<ul class="org-ul">
<li>5.2 System management
<ul class="org-ul">
<li>5.2.1 Build instructions for IPMItool
<ul class="org-ul">
<li>IPMI – это набор спецификаций, регламентирующих, как общаться и что предоставлять.
<ul class="org-ul">
<li>BMC – это обертка из железа для работы IPMI. Представляет собой
одноплатный (system on а chip) с щупальцами в сенсорах основного.</li>
<li>IBM IMM (Integrated Management Module)</li>
</ul></li>
</ul></li>
</ul></li>
<li>5.3 xCAT overview</li>
<li>5.4 xCAT Management Node</li>
<li>5.5 xCAT Node Discovery</li>
<li>5.6 xCAT Compute Nodes</li>
<li>5.7 xCAT Login Nodes</li>
</ul></li>
<li>Chapter 6. Application development and tuning
<ul class="org-ul">
<li>6.2 Engineering and Scientific Subroutine Library</li>
<li>6.4 Using POWER8 vectorization</li>
<li>6.5 Development models
<ul class="org-ul">
<li>6.5.1 MPI programs with IBM Parallel Environment</li>
<li>6.5.2 CUDA C programs with the NVIDIA CUDA Toolkit</li>
<li>6.5.3 Hybrid MPI and CUDA programs with IBM Parallel Environment</li>
<li>6.5.4 OpenMP programs with the IBM Parallel Environment</li>
<li>6.5.5 OpenSHMEM programs with the IBM Parallel Environment</li>
<li>6.5.6 Parallel Active Messaging Interface programs</li>
</ul></li>
<li>6.6 GPU tuning</li>
<li>6.7 Tools for development and tuning of applications
<ul class="org-ul">
<li>6.7.1 The Parallel Environment Developer Edition</li>
<li>6.7.2 IBM PE Parallel Debugger</li>
<li>6.7.3 Eclipse for Parallel Application Developers</li>
<li>6.7.4 NVIDIA Nsight Eclipse Edition for CUDA C/C++</li>
<li>6.7.5 Command-line tools for CUDA C/C++</li>
</ul></li>
</ul></li>
<li>Chapter 8. Cluster monitoring
<ul class="org-ul">
<li>8.1 IBM Spectrum LSF tools for monitoring</li>
<li>8.2 nvidia-smi tool for monitoring GPU</li>
</ul></li>
<li>Appendix A. Applications and performance
<ul class="org-ul">
<li>Application software
<ul class="org-ul">
<li>Bioinformatics</li>
<li>OpenFOAM</li>
<li>NAMD program</li>
</ul></li>
<li>Effects of basic performance tuning techniques
<ul class="org-ul">
<li>The performance impact of a rational choice of an SMT mode</li>
<li>The impact of optimization options on performance</li>
<li>Summary of favorable modes and options for applications from the NPB suite</li>
<li>The importance of binding threads to logical processors</li>
</ul></li>
<li>General methodology of performance benchmarking</li>
<li>Sample code for the construction of thread affinity strings</li>
<li>ESSL performance results</li>
</ul></li>
</ul></li>
</ul>
</div>
</div>
</div>
</div>

<div id="outline-container-orgheadline19" class="outline-2">
<h2 id="orgheadline19"><span class="section-number-2">5</span> Литература</h2>
<div class="outline-text-2" id="text-5">
<ul class="org-ul">
<li>Ломоносов
<ul class="org-ul">
<li><a href="http://www.msu.ru/lomonosov/science/computer.html">msu.ru Суперкомпьютер "Ломоносов"</a></li>
<li><a href="https://parallel.ru/cluster/lomonosov.html">parallel lomonosov</a></li>
<li><a href="http://hpc.msu.ru/">http://hpc.msu.ru/</a></li>
<li>Clustrx T-Platforms Edition (Linux)</li>
</ul></li>

<li>Результаты оценочного тестирования отечественной высокоскоростной
коммуникационной сети Ангара, 2016
<ul class="org-ul">
<li><a href="http://russianscdays.org/files/pdf16/626.pdf">http://russianscdays.org/files/pdf16/626.pdf</a></li>
<li>В статье представлены результаты сравнительного оценочного
тестирования 36-узлового вычислительного кластера «Ангара-К1»,
оснащенного адаптерами коммуникационной сети Ангара, и
суперкомпьютера МВС-10П с сетью InfiniBand 4x FDR, установленного
в МСЦ РАН.
<ul class="org-ul">
<li>Ключевые слова: высокоскоростная сеть, интерконнект, Ангара,
InfiniBand FDR, HPCG, HPL, NPB, ПЛАВ</li>
</ul></li>
</ul></li>

<li>Исследование возможностей GPU в высокопроизводительных вычислениях [<a href="#kulikov2016issledovanie">2</a>]
<ul class="org-ul">
<li>Исходный код Linpack для GPU (CUDA-enabled version of HPL 2.0
optimized for Tesla 20-series GPU Fermi version 1.5) соответствует
алгоритмам версии 2.0 для CPU. В качестве подключаемых
библиотек Линейной Алгебры (BLAS) для HPL версии 2.0 использовался
пакет INTEL MKL из Intel Composer 2013.3.163. В качестве
подключаемых библиотек для использования NVIDIA Tesla K20Xm
использовался пакет NVIDIA CUDA 5. Для сборки программного
обеспечения Linpack использовался компилятор GCC 4.4.</li>
</ul></li>

<li>МОДЕЛИРОВАНИЕ ОБРАБОТКИ ЗАПРОСОВ НА ГИБРИДНЫХ ВЫЧИСЛИТЕЛЬНЫХ
СИСТЕМАХ С МНОГОЯДЕРНЫМИ СОПРОЦЕССОРАМИ И ГРАФИЧЕСКИМИ УСКОРИТЕЛЯМИ
<ul class="org-ul">
<li><a href="http://agora.guru.ru/abrau2013/pdf/202.pdf">http://agora.guru.ru/abrau2013/pdf/202.pdf</a></li>
</ul></li>

<li>Конфигурирование и тестирование производительности вычислительного
кластера на базе неоднородных многоядерных узлов
<ul class="org-ul">
<li><a href="http://moluch.ru/archive/93/20529/">http://moluch.ru/archive/93/20529/</a></li>
<li>В работе рассматривается выбор оборудования, программного
обеспечения, вопросы настройки и тестирования производительности
вычислительного кластера на базе небольшого числа неоднородных
многоядерных серверных узлов. Оценка производительности
выполняется как с помощью традиционного теста Linpack,
оптимизированного для конкретной конфигурации, так и с точки
зрения производительности при решении прикладных задач.
<ul class="org-ul">
<li>Ключевые слова: вычислительный кластер, кластер рабочих станций
(COW), массивно-параллельная обработка (MPP),
High-perfomancecomputing (HPC), GPU, архитектура
IntelManyIntegratedCore (MIC), IntelXeonPhi, IntelXeonE5,
Cent'OS, MPI, Linpack, IntelMKL, SIESTA, ANSYS, FlowVision,
Mathematica, FRUND, Top50.</li>
</ul></li>
</ul></li>

<li><a href="http://samag.ru/archive/article/3202">Исследование воздействия некоторых параметров теста LINPACK</a> (2016, Ломоносов)
<ul class="org-ul">
<li>При решении систем линейных алгебраических уравнений на
современных гибридных (CPU + GPU) кластерах перед пользователем
возникает задача выбора значений ряда параметров, оказывающих
существенное, но неочевидное влияние на производительность
вычислений и, как следствие, на временные, а следовательно, и
технические затраты на решение задачи. Существующие рекомендации
по выбору значений этих параметров носят оценочный характер и не
гарантируют достижения максимальной производительности вычислений
при заданной размерности системы линейных алгебраических
уравнений. Целью работы является экспериментальное исследование
влияния значений параметров Nb и CUDA_DGEMM_SPLIT теста LINPACK,
представляющего собой решение модельной системы линейных
алгебраических уравнений методом LU-разложения, на
производительность вычислений на гибридных узлах кластера
«Ломоносов» (МГУ, Москва). Получены рекомендательные данные
значений параметров Nb и CUDA_DGEMM_SPLIT в зависимости от
размерности системы линейных алгебраических уравнений для
достижения максимальной производительности при решении системы
линейных алгебраических уравнений методом LU-разложения на
гибридных вычислительных узлах кластера «Ломоносов». Построенная в
работе рекомендательная таблица для кластера «Ломоносов» позволяет
однозначно выбирать значения параметров Nb и CUDA_DGEMM_SPLIT в
зависимости от размерности системы линейных алгебраических
уравнений для достижения максимальной производительности
вычислений.</li>
<li>В настоящей статье приведены результаты дальнейших исследований,
касающиеся эффективности вычислений теста LINPACK при изменении
параметров Nb (величина размерности логических блоков Nb × Nb, на
которые разбивается исходная матрица) и CUDA_DGEMM_SPLIT (процент
работы, загружаемой в GPU для умножения матриц сдвойной
точностью), тесно связанных с аппаратной структурой вычислительных
узлов кластера.</li>
<li>Основное отличие LINPACK для GPU заключается в том, что в исходном
коде HPL 2.0 ядра и CPU, и GPU с небольшими модификациями или без
модификаций используются совокупно (эффективность совокупности CPU
и GPU превышает сумму их индивидуальных эффективностей):
<ul class="org-ul">
<li>Библиотека узла прерывает вызовы к DGEMM и DTRSM ивыполняет
их одновременно на ядрах CPU иGPU, где:
<ul class="org-ul">
<li>DGEMM – Double-precision General Matrix Multiply – умножение
матриц с двойной точностью;</li>
<li>DTRSM – Double-precision TRiangular Solve Multiple (solution
of the triangular systems of linear equations) – решение
треугольных систем линейных уравнений с двойной точностью.</li>
</ul></li>
<li>Использование pinned memory для быстрых передач данных по
компьютерной шине PCI Express со скоростью до 5.7 GB/s на слотах
x16 gen2.</li>
</ul></li>
</ul></li>
</ul>


<ul class="org-ul">
<li>Тестирование суперкомпьютеров
<ul class="org-ul">
<li>Учебный курс «Технологии построения и использования кластерных систем» 2007
<ul class="org-ul">
<li><a href="http://www.hpcc.unn.ru/multicore/materials/cluster/performance_test.pdf">http://www.hpcc.unn.ru/multicore/materials/cluster/performance_test.pdf</a></li>
<li>Необходимость тестирования
<ul class="org-ul">
<li>самодиагностика
<ul class="org-ul">
<li>Cluster Management System (CMS)</li>
</ul></li>
<li>существенные характеристики</li>
</ul></li>
</ul></li>
<li>Комплексная методика тестирования производительности суперкомпьютеров [<a href="#gorbunov2013komplexnaya">1</a>]
<ul class="org-ul">
<li><a href="http://num-meth.srcc.msu.ru/zhurnal/tom_2013/pdf/v14r216.pdf">http://num-meth.srcc.msu.ru/zhurnal/tom_2013/pdf/v14r216.pdf</a></li>
<li>DIS-задачи (Data Intensive Systems), отличающиеся плохой
пространственно-временн́ой локализацией и непредсказуемость ю
адресов обращений к памяти, а также высокой интенсивностью
операций с памятью в сравнении с вычислительными</li>
<li>СF-задачи (Cache-Friendly, “дружественные” к кэш-памяти),
имеющие хорошую пространственно-временн́ую локализацию и
предсказуемость адресов обращений к памяти;</li>
<li>Это должно повысить толерантность приложения к задержкам
выполнения операций с памятью, предоставив одновременно
возможность комфортной работы с огромной глобально адресуемой
памятью, отображаемой на физические памяти множества узлов.</li>
<li>Динамика трех рейтинговых списков демонстрирует актуальность
характеристик, связанных с выполнением операций с
памятью. Отметим, что такие характеристики были заложены в
основу базовой методики оценочного тестирования, применяемой в
Центре, которая будет рассмотрена далее. Однако сначала
остановимся на разных схемах использования методик оценочного
тестирования.</li>
</ul></li>
<li><a href="http://www.siam.org/meetings/ex14/02-oberlin-slides.pdf">Accelerating Exascale</a>
<ul class="org-ul">
<li>Accelerating Exascale
<ul class="org-ul">
<li>How the End of Moore’s Law Scaling is Changing the Machines
You Use, the Way You Code, and the Algorithms You Use</li>
</ul></li>
<li>Pascal NVLink</li>
<li>LINPACK vs. Real Apps</li>
<li>Programmers, Tools, and Architecture Need to Play Their Positions</li>
<li>Exascale (25x) is Within Reach
<ul class="org-ul">
<li>Requires clever circuits and ruthlessly-efficient architecture
<ul class="org-ul">
<li>Moore’s Law cannot be relied upon</li>
</ul></li>
<li>Need to simplify programming and automate mapping
<ul class="org-ul">
<li>“MPI + X” is only a step in the right direction</li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>

<li>ИНТУИТ <a href="http://www.intuit.ru/studies/courses/3457/699/lecture/14133?page=2">Лекция 2</a>: Методы оценки вычислительных характеристик задач
предметной области и поддерживающих их аппаратных платформ</li>

<li>Особенности измерения основных характеристик вычислительных систем
<ul class="org-ul">
<li>Сравнение компьютеров между собой обычно начинают с оценки их
производительности. Это потребовало введения соответствующих единиц
измерения производительности и разработки стандартных методов ее
оценки.</li>
<li>Тест LINPACK используется при составлении рейтинга самых
высокопроизводительны компьютеров мира. Результаты размещаются на
сайте <a href="http://www.top500.org/">http://www.top500.org/</a>.</li>
</ul></li>
</ul>

<p>
В основе используемых в LINPACK алгоритмов лежит метод декомпозиции,
широко применяемый при высокопроизводительных
вычислениях. Достоинством тестов LINPACK является их
структурированность. Для реализации элементарных операций над
векторами, которые включаю умножение векторов на скаляр, сложение
векторов, скалярное произведение векторов выделяется базовый уровень
системы, называемый BLAS (Basic Linear Algebra Subprograms).
</p>

<ul class="org-ul">
<li><a href="http://s1.fksis.ru/abc/%D0%A2%D0%B5%D0%BE%D1%80%D0%B8%D1%8F/content/ak2/theme13.htm">13</a>. Оценка производительности вычислительных систем</li>
</ul>
</div>
</div>
</div>
</body>
</html>
