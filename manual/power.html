<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="ru" xml:lang="ru">
<head>
<!-- 2017-05-16 Вт. 17:59 -->
<meta  http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta  name="viewport" content="width=device-width, initial-scale=1" />
<title>Руководство пользователя кластера OpenPOWER ВЦ ДВО РАН</title>
<meta  name="generator" content="Org-mode" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  pre.src-sh:before    { content: 'sh'; }
  pre.src-bash:before  { content: 'sh'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-R:before     { content: 'R'; }
  pre.src-perl:before  { content: 'Perl'; }
  pre.src-java:before  { content: 'Java'; }
  pre.src-sql:before   { content: 'SQL'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  /*]]>*/-->
</style>
<link rel="stylesheet" type="text/css" href="../org-html-themes/styles/readtheorg/css/htmlize.css"/>
<link rel="stylesheet" type="text/css" href="../org-html-themes/styles/readtheorg/css/readtheorg.css"/>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js"></script>
<script type="text/javascript" src="../org-html-themes/styles/lib/js/jquery.stickytableheaders.min.js"></script>
<script type="text/javascript" src="../org-html-themes/styles/readtheorg/js/readtheorg.js"></script>
<style type="text/css">.org-src-name{ text-align: right; }</style>
<style type="text/css">.outline-2{ margin-top: 60px; }</style>

<style type="text/css">
  pre.src:before {
    top: -5px;
  }
</style>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2013 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
</head>
<body>
<div id="content">
<h1 class="title">Руководство пользователя кластера OpenPOWER ВЦ ДВО РАН</h1>
<div id="table-of-contents">
<h2>&#1057;&#1086;&#1076;&#1077;&#1088;&#1078;&#1072;&#1085;&#1080;&#1077;</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#registration">1. Регистрация</a></li>
<li><a href="#login">2. Вход в систему</a>
<ul>
<li><a href="#login_windows">2.1. Вход с Windows-машины</a></li>
<li><a href="#login_linux">2.2. Вход с терминала Linux</a></li>
</ul>
</li>
<li><a href="#copy_files">3. Копирование файлов</a></li>
<li><a href="#work_cluster">4. Работа на кластере</a>
<ul>
<li><a href="#navigation">4.1. Навигация</a></li>
<li><a href="#editing">4.2. Редактирование файлов</a></li>
<li><a href="#compilation">4.3. Компиляция программ</a>
<ul>
<li><a href="#notes_dev">4.3.1. Замечания по разработке программ на отдельной машине</a></li>
</ul>
</li>
<li><a href="#run_tasks">4.4. Запуск задач</a>
<ul>
<li><a href="#dispatch_tasks">4.4.1. Диспетчеризация задач</a></li>
<li><a href="#queues">4.4.2. Система очередей</a></li>
<li><a href="#queue_tasks">4.4.3. Постановка задачи в очередь</a></li>
<li><a href="#interactive">4.4.4. Запуск интерактивных программ</a></li>
<li><a href="#nonparallel">4.4.5. Запуск непараллельных программ</a></li>
<li><a href="#user_tasks">4.4.6. Состояние пользовательских задач</a></li>
<li><a href="#stop_tasks">4.4.7. Остановка задач</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#monitoring">5. Мониторинг</a>
<ul>
<li><a href="#web_interface">5.1. Web-интерфейс</a></li>
<li><a href="#console">5.2. Консоль</a></li>
</ul>
</li>
<li><a href="#reference">6. Справочная информация</a></li>
</ul>
</div>
</div>
<p class="verse">
Версия: <b>0.1</b><br  />
Дата: <b>16.05.2017</b><br  />
</p>

<p>
<br  />
<br  />
Данное руководство содержит минимально необходимый объем информации
для работы на кластере ВЦ ДВО РАН: описание процесса регистрации,
сведения по работе в ОС Linux (вход в систему, работа с каталогами и
файлами, мониторинг) и работе с MPI программами и не параллельными
программами на кластере (компиляция, запуск, остановка, работа с
очередями). В тексте под термином параллельная программа
подразумеваются только MPI программы.
</p>

<p>
Команды и переменные командного
интерпретатора, названия программ, листинги, непосредственный
ввод/вывод консоли выделены моноширинным шрифтом.
</p>

<p>
Вопросы относительно работы кластера следует отправлять на <i>e-mail</i>:
<a href="mailto:support@hpc.febras.net">support@hpc.febras.net</a>.
</p>

<p>
Вопросы относительно этого документа (ошибки, неточности, предложения)
можно отправлять на <i>e-mail</i>: <a href="mailto:support@hpc.febras.net">support@hpc.febras.net</a>.
</p>

<div id="text-table-of-contents">
<ul>
<li><a href="#registration">1. Регистрация</a></li>
<li><a href="#login">2. Вход в систему</a>
<ul>
<li><a href="#login_windows">2.1. Вход с Windows-машины</a></li>
<li><a href="#login_linux">2.2. Вход с терминала Linux</a></li>
</ul>
</li>
<li><a href="#copy_files">3. Копирование файлов</a></li>
<li><a href="#work_cluster">4. Работа на кластере</a>
<ul>
<li><a href="#navigation">4.1. Навигация</a></li>
<li><a href="#editing">4.2. Редактирование файлов</a></li>
<li><a href="#compilation">4.3. Компиляция программ</a>
<ul>
<li><a href="#notes_dev">4.3.1. Замечания по разработке программ на отдельной машине</a></li>
</ul>
</li>
<li><a href="#run_tasks">4.4. Запуск задач</a>
<ul>
<li><a href="#dispatch_tasks">4.4.1. Диспетчеризация задач</a></li>
<li><a href="#queues">4.4.2. Система очередей</a></li>
<li><a href="#queue_tasks">4.4.3. Постановка задачи в очередь</a></li>
<li><a href="#interactive">4.4.4. Запуск интерактивных программ</a></li>
<li><a href="#nonparallel">4.4.5. Запуск непараллельных программ</a></li>
<li><a href="#user_tasks">4.4.6. Состояние пользовательских задач</a></li>
<li><a href="#stop_tasks">4.4.7. Остановка задач</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#monitoring">5. Мониторинг</a>
<ul>
<li><a href="#web_interface">5.1. Web-интерфейс</a></li>
<li><a href="#console">5.2. Консоль</a></li>
</ul>
</li>
<li><a href="#reference">6. Справочная информация</a></li>
</ul>
</div>

<div id="outline-container-orgheadline1" class="outline-2">
<h2 id="registration"><a id="orgheadline1"></a><span class="section-number-2">1</span> Регистрация</h2>
<div class="outline-text-2" id="text-registration">
<p>
Регистрация пользователей на кластере происходит через систему ЦКП
<a href="http://ckp.ccfebras.ru/">http://ckp.ccfebras.ru/</a>.
</p>
</div>
</div>

<div id="outline-container-orgheadline4" class="outline-2">
<h2 id="login"><a id="orgheadline4"></a><span class="section-number-2">2</span> Вход в систему</h2>
<div class="outline-text-2" id="text-login">
<p>
Для работы с системой пользователь должен иметь свою учетную запись на
управляющем узле кластера. Регистрация пользователя на кластере
происходит в соответствии с предыдущей частью руководства. После
регистрации пользователь получает свое имя (логин), пароль и домашнюю
директорию. Если имя пользователя, например, будет <b>user</b>, то домашняя
папка находится в <b>/home/user</b>.
</p>

<p>
При первом входе в систему предлагается сменить пароль. Требования к
новому паролю: он должен быть достаточной длины, содержать хотя бы 1
цифру и 1 заглавную букву. Директория <code>~/.ssh</code> содержит пару ключей
(<i>id_rsa</i> и <i>authorized_keys</i>) для доступа к узлам кластера. При
добавлении личного ssh-ключа, необходимо добавить открытый ключ в файл
<i>authorized_keys</i> через перенос строки (т.е. после уже имеющегося одного
открытого ключа).
</p>

<p>
Пользователи имеют возможность работать на кластере с любой машины,
находящейся в сети института и интернет. Для входа в систему
пользователю необходим адрес сервера (<a href="http://jupiter.febras.net/">jupiter.febras.net</a>), а также
имя и пароль, полученные при регистрации.
</p>
</div>

<div id="outline-container-orgheadline2" class="outline-3">
<h3 id="login_windows"><a id="orgheadline2"></a><span class="section-number-3">2.1</span> Вход с Windows-машины</h3>
<div class="outline-text-3" id="text-login_windows">
<p>
Работа с системой осуществляется по безопасному протоколу SSH при помощи какого-либо
ssh-клиента. Клиент должен поддерживать протокол версии 2. Рекомендуется использовать 
<a href="https://www.chiark.greenend.org.uk/~sgtatham/putty/">PuTTY</a>.
Эта программа является свободно распространяемой и проста в использовании.
</p>

<p>
После запуска программы (рис. 1) пользователь должен выбрать протокол
ssh и в поле «Host Name (or IP address)» указать адрес
сервера. Нажатие на «Open» приведет к отправке запроса на
подключение. В случае успешного подключения к серверу будет предложено
ввести имя (логин), а затем и пароль.
</p>


<div id="orgparagraph1" class="figure">
<p><img src="./putty.png" alt="putty.png" />
</p>
<p><span class="figure-number">&#1056;&#1080;&#1089;. 1.:</span> Окно ssh-клиента PuTTY</p>
</div>

<p>
При вводе пароля символы на экране не отображаются. Если все введено
правильно, то пользователь автоматически окажется в своей домашней
директории. Этот каталог доступен пользователю с любого узла кластера.
</p>

<div class="note">
<p>
<b>Примечание</b>. На кластере существует единое дисковое пространство для
директорий <b>/opt</b> (только чтение) и <b>/home</b>.  Все узлы используют
дисковый массив сервера посредством сетевой файловой системы NFS. Файл
записанный на одном из узлов кластера автоматически становится
доступен на любом другом.
</p>

</div>

<p>
Работа в ssh-сессии происходит в терминальном (текстовом, консольном)
режиме. Необходимо помнить, что консоль Linux, в отличии от Windows,
различает регистр вводимых символов, то есть <code>mydoc.txt</code> и <code>mydoc.TXT</code> не
одно и то же. После входа на экране отображается консоль командного
интерпретатора в следующем формате имя_пользователя@машина текущий_каталог:
</p>

<div class="org-src-container">

<pre class="src src-raw">[user@jupiter ~]$
</pre>
</div>
</div>
</div>

<div id="outline-container-orgheadline3" class="outline-3">
<h3 id="login_linux"><a id="orgheadline3"></a><span class="section-number-3">2.2</span> Вход с терминала Linux</h3>
<div class="outline-text-3" id="text-login_linux">
<p>
В любой дистрибутив ОС Linux входит терминальный ssh-клиент (обычно
OpenSSH). Минимальный формат команды для подключения к кластеру таков:
</p>

<div class="org-src-container">

<pre class="src src-raw">[user@localhost ~]$ ssh jupiter.febras.net -l имя_пользователя
</pre>
</div>
</div>
</div>
</div>

<div id="outline-container-orgheadline5" class="outline-2">
<h2 id="copy_files"><a id="orgheadline5"></a><span class="section-number-2">3</span> Копирование файлов</h2>
<div class="outline-text-2" id="text-copy_files">
<p>
См. соответсвующий раздел <a href="http://lits.ccfebras.ru/assets/files/user_guide.pdf">руководства кластера версии 5.0</a>.
</p>
</div>
</div>

<div id="outline-container-orgheadline18" class="outline-2">
<h2 id="work_cluster"><a id="orgheadline18"></a><span class="section-number-2">4</span> Работа на кластере</h2>
<div class="outline-text-2" id="text-work_cluster">
</div>

<div id="outline-container-orgheadline6" class="outline-3">
<h3 id="navigation"><a id="orgheadline6"></a><span class="section-number-3">4.1</span> Навигация</h3>
<div class="outline-text-3" id="text-navigation">
<p>
См. соответсвующий раздел <a href="http://lits.ccfebras.ru/assets/files/user_guide.pdf">руководства кластера версии 5.0</a>.
</p>
</div>
</div>

<div id="outline-container-orgheadline7" class="outline-3">
<h3 id="editing"><a id="orgheadline7"></a><span class="section-number-3">4.2</span> Редактирование файлов</h3>
<div class="outline-text-3" id="text-editing">
<p>
См. соответсвующий раздел <a href="http://lits.ccfebras.ru/assets/files/user_guide.pdf">руководства кластера версии 5.0</a>.
</p>
</div>
</div>

<div id="outline-container-orgheadline9" class="outline-3">
<h3 id="compilation"><a id="orgheadline9"></a><span class="section-number-3">4.3</span> Компиляция программ</h3>
<div class="outline-text-3" id="text-compilation">
<p>
На кластере (на 16.05.2017)
 поддерживаются следующие компиляторы языков
программирования для архитектуры ppc64le:
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides" align="center">
<caption class="t-above"><span class="table-number">&#1058;&#1072;&#1073;&#1083;&#1080;&#1094;&#1072; 1.:</span> Компиляторы на кластере</caption>

<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">Компилятор</td>
<td class="org-left">Путь к файлу компилятора</td>
<td class="org-left">Язык</td>
</tr>

<tr>
<td class="org-left">GNU C 4.8.5</td>
<td class="org-left">/usr/bin/gcc</td>
<td class="org-left">C</td>
</tr>

<tr>
<td class="org-left">GNU C++ 4.8.5</td>
<td class="org-left">/usr/bin/g++</td>
<td class="org-left">C++</td>
</tr>

<tr>
<td class="org-left">GNU Fortran 4.8.5</td>
<td class="org-left">/usr/bin/gfortran</td>
<td class="org-left">Fortran 90</td>
</tr>

<tr>
<td class="org-left">IBM XL Fortran 15.1.5</td>
<td class="org-left">/usr/bin/xlf</td>
<td class="org-left">Fortran 77</td>
</tr>

<tr>
<td class="org-left">IBM XL Fortran 15.1.5</td>
<td class="org-left">/usr/bin/xlf90</td>
<td class="org-left">Fortran 90</td>
</tr>

<tr>
<td class="org-left">IBM XL C 13.1.5</td>
<td class="org-left">/usr/bin/xlc</td>
<td class="org-left">C</td>
</tr>

<tr>
<td class="org-left">IBM XL C++ 13.1.5</td>
<td class="org-left">/usr/bin/xlc++</td>
<td class="org-left">C++</td>
</tr>

<tr>
<td class="org-left">NVIDIA Cuda 8.0.61</td>
<td class="org-left">/usr/local/cuda/bin/nvcc</td>
<td class="org-left">C/C++</td>
</tr>
</tbody>
</table>

<p>
Компиляторы GNU и IBM XL находятся в каталоге, доступном для всех
пользователей. Поэтому, например, при вызове команды:
</p>
<div class="org-src-container">

<pre class="src src-raw">[user@jupiter ~]$ xlc
</pre>
</div>
<p>
будет запускаться компилятор IBM XL C 13.1.5. В переменной среды
«LD_LIBRARY_PATH» также указаны пути к библиотекам этих компиляторов.
</p>

<p>
На кластере используется система модулей окружения (<a href="http://modules.sourceforge.net/">Environment
Modules</a>). Загруженные модули можно посмотреть с помощью команды:
</p>
<div class="org-src-container">

<pre class="src src-raw">[user@jupiter ~]$ module list
Currently Loaded Modulefiles:
  1) pbs            2) cuda           3) essl           4) spectrum_mpi
</pre>
</div>
<p>
Видно, что в отличии от компиляторов GNU и IBM XL, компилятор cuda
подключается при помощи соответствующего модуля. Среди подключенных
по-умолчанию модулей находится и <i>spectrum_mpi</i>.
</p>

<p>
В качестве реализации MPI библиотеки на кластере (на
16.05.2017)
поддерживается IBM Spectrum MPI. Кроме того, можно использовать любую
из имеющихся дополнительных реализаций, подключив необходимый модуль:
</p>

<div class="org-src-container">

<pre class="src src-raw">module unload spectrum_mpi
module load openmpi/gcc/1.10.6/4.8.5
</pre>
</div>
<p>
Все реализациии MPI конфликтуют между собой, поэтому необходимо
предварительно отключать альтернативный модуль. После подключения
нужного модуля, пути к данной библиотеке добавляются в переменные
среды «PATH» и «LD_LIBRARY_PATH».
</p>

<p>
Список доступных модулей можно увидеть с помощью команды:
</p>
<div class="org-src-container">

<pre class="src src-raw">[user@jupiter ~]$ module avail

------------------------------------------ /usr/share/Modules/modulefiles -------------------------------------------
dot         module-git  module-info modules     null        use.own

------------------------------------------------- /etc/modulefiles --------------------------------------------------
cuda                      openmpi/gcc/2.1.0/4.8.5   openmpi/xl/2.1.0          spectrum_mpi
essl                      openmpi/pgi/1.10.2/2016   pbs
openmpi/gcc/1.10.6/4.8.5  openmpi/xl/1.10.6         pgi/16.10(default)
openmpi/gcc/2.0.2a1/4.8.5 openmpi/xl/2.0.2a1        pgi/2016
</pre>
</div>

<p>
Для компиляции mpi программ лучше всего использовать обёртки к
компиляторам, чем вручную прописывать для этого специальные
флаги. Так, например, чтобы скомпилировать mpi программу, написанную
на языке Fortran, нужно воспользоваться оберткой <code>mpifort</code>. Данная
команда вызовет компилятор IBM XL Fortran (при условии, что подключен
модуль spectrum_mpi), с указанием всех необходимых флагов.
</p>

<p>
Распишем соответствие между обертками и соответствующими им компиляторами:
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides" align="center">
<caption class="t-above"><span class="table-number">&#1058;&#1072;&#1073;&#1083;&#1080;&#1094;&#1072; 2.:</span> Соответствие между обертками MPI и компиляторами</caption>

<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">Обертка</td>
<td class="org-left">XL</td>
<td class="org-left">GNU</td>
</tr>

<tr>
<td class="org-left">mpicc</td>
<td class="org-left">xlc_r</td>
<td class="org-left">gcc</td>
</tr>

<tr>
<td class="org-left">mpifort</td>
<td class="org-left">xlf_r</td>
<td class="org-left">gfortran</td>
</tr>

<tr>
<td class="org-left">mpif77</td>
<td class="org-left">xlf_r</td>
<td class="org-left">gfortran</td>
</tr>

<tr>
<td class="org-left">mpif90</td>
<td class="org-left">xlf90_r</td>
<td class="org-left">gfortran</td>
</tr>

<tr>
<td class="org-left">mpic++</td>
<td class="org-left">xlC_r</td>
<td class="org-left">g++</td>
</tr>

<tr>
<td class="org-left">mpicxx</td>
<td class="org-left">xlC_r</td>
<td class="org-left">g++</td>
</tr>
</tbody>
</table>

<p>
Для того, чтобы посмотреть какие опции компилятора указываются при
вызове обертки, можно воспользоваться следующей командой:
</p>

<div class="org-src-container">

<pre class="src src-raw">[user@jupiter ~]$ mpicc -show
gcc -I/opt/soft/openmpi/1.10.6/gcc/include -pthread -Wl,-rpath
-Wl,/opt/soft/openmpi/1.10.6/gcc/lib -Wl,--enable-new-dtags
-L/opt/soft/openmpi/1.10.6/gcc/lib -lmpi
</pre>
</div>

<p>
Как видно из данного вывода, единственными опциями, которые может
потребоваться указать при вызове компилятора, могут оказаться опции
оптимизации.
</p>
</div>

<div id="outline-container-orgheadline8" class="outline-4">
<h4 id="notes_dev"><a id="orgheadline8"></a><span class="section-number-4">4.3.1</span> Замечания по разработке программ на отдельной машине</h4>
<div class="outline-text-4" id="text-notes_dev">
<p>
Практически все реализации MPI поддерживают запуск параллельных
приложений в режиме эмуляции на отдельно взятой рабочей станции. Это
можно делать как на Linux, так и Windows машинах.
</p>

<p>
В Linux рекомендуется использовать пакет OpenMPI, а для создания MPI
приложений на Windows машинах можно использовать пакет MPICH в версии
для Windows. Для успешного портирования программ с Windows на Linux не
следует использовать расширения предоставляемые средами
программирования, такими как Visual Studio и Borland Builder.
</p>

<p>
Подготовленные исходные коды программ лучше всего компилировать на
кластере.
</p>
</div>
</div>
</div>

<div id="outline-container-orgheadline17" class="outline-3">
<h3 id="run_tasks"><a id="orgheadline17"></a><span class="section-number-3">4.4</span> Запуск задач</h3>
<div class="outline-text-3" id="text-run_tasks">
</div>

<div id="outline-container-orgheadline10" class="outline-4">
<h4 id="dispatch_tasks"><a id="orgheadline10"></a><span class="section-number-4">4.4.1</span> Диспетчеризация задач</h4>
<div class="outline-text-4" id="text-dispatch_tasks">
<p>
Для диспетчеризации задач на кластере используется система PBS
Pro. С её помощью пользователь может отправлять свои задачи на
исполнение, снимать их с исполнения и получать информацию по текущему
статусу задачи.
</p>

<p>
Данная система построена на основе очередей, где под очередью
понимается набор пользовательских процессов (программ, задач)
выполняющихся в рамках системы диспетчеризации. Каждой очереди
сопоставлен ряд атрибутов, в зависимости от которых к задаче будут
применены те или иные действия. Типичными атрибутами являются название
(идентификатор) очереди, её приоритет, доступные ресурсы, количество
задач. В общем случае термин очередь не означает, то что программы в
ней будут выполняться строго последовательно.
</p>

<p>
Чтобы поставить задачу на исполнение, пользователь должен добавить ее
при помощи команды <code>qsub</code> в какую-либо очередь. Очереди отличаются
друг от друга совокупностью ресурсов, которыми они обладают.
</p>
</div>
</div>

<div id="outline-container-orgheadline11" class="outline-4">
<h4 id="queues"><a id="orgheadline11"></a><span class="section-number-4">4.4.2</span> Система очередей</h4>
<div class="outline-text-4" id="text-queues">
<p>
На данный момент действует 1 очередь: <i>workq</i>.
</p>

<p>
Для получения информации об очередях, можно выполнить команду <code>qstat -q</code>:
</p>
<div class="org-src-container">

<pre class="src src-raw">[eab@jupiter install]$ qstat -q

server: jupiter1

Queue            Memory CPU Time Walltime Node   Run   Que   Lm  State
---------------- ------ -------- -------- ---- ----- ----- ----  -----
workq              --      --       --     --      0     0   --   E R
                                               ----- -----
                                                   0     0
</pre>
</div>

<p>
<b>Queue</b> – имя очереди; <b>Run</b> – число выполняемых задач; <b>Que</b> – число задач, ожидающих начала выполнения.
Команда <code>qstat -Qf имя_очереди</code>  позволяет получить информацию о конкретной очереди.
</p>
</div>
</div>

<div id="outline-container-orgheadline12" class="outline-4">
<h4 id="queue_tasks"><a id="orgheadline12"></a><span class="section-number-4">4.4.3</span> Постановка задачи в очередь</h4>
<div class="outline-text-4" id="text-queue_tasks">
<p>
Для постановки задачи в очередь на исполнение используется команда
<code>qsub</code>. Данная команда принимает в качестве параметра имя скрипта, в
котором описываются требуемые задачей ресурсы и указываются команды,
исполняемые при запуске. Рассмотрим пример, иллюстрирующий запуск
ранее скомпилированной программы на 1 чанке (некоторой виртуальной
части) кластера, с использованием 4 mpi процессов, выделением 4
процессоров и 1 GPU на этом чанке:
</p>

<div class="org-src-container">

<pre class="src src-raw">[user@jupiter mpi_test]$ cat mpi_test.qsub
#PBS -k oe
#PBS -l select=1:mpiprocs=4:ncpus=4:ngpus=1
#PBS -l place=shared
#PBS -r n
#PBS -M user@mail.com
#PBS -m abe
#PBS -q workq
#PBS -N mpi_test
#!/bin/sh

cd /home/user/test/mpi_test

module unload spectrum_mpi &amp;&amp; module load openmpi/gcc/1.10.6/4.8.5
mpirun ./mpi

exit 0
[user@jupiter mpi_test]$ qsub mpi_test.qsub
66330.jupiter1
</pre>
</div>

<p>
Если команда выполнена успешно, то на экране отобразится идентификатор
задачи (в данном случае это <i>66330.jupiter1</i>), в противном случае
появится сообщение об ошибке. Ошибки пользовательской программы
(неправильная компиляция и т.п.) проявятся только при переходе задачи
к активному состоянию.
</p>

<div class="note">
<p>
<b>Примечание.</b> Весь вывод программы в стандартный поток и в поток ошибок
перенаправляется в файлы, находящиеся в домашней директории пользователя.
Названия таких файлов имеют формат <code>имя_задачи.(e/o)порядковый_номер</code>. Для
запущенной задачи это будут: <i>mpi_test.e66330</i> – для потока ошибок и
<i>mpi_test.o66330</i> – для стандартного потока вывода.
</p>

</div>

<p>
Прокомментируем каждую из строчек скрипта <code>mpi_test.qsub</code>:
</p>

<p>
<code>#PBS -k oe</code> — указание сброса потока вывода (o) и потока ошибок (e)<br  />
<code>#PBS -l select=1:mpiprocs=4:ncpus=4:ngpus=1</code> — требуемое количество чанков (1); количество mpi процессов (4), количество выделяемых процессоров (4) и количество выделяемых GPU (1) на каждом чанке<br  />
<code>#PBS -l place=shared</code> — использование узла вместе с другими задачами; <b>shared</b> — совместное использование, <b>excl</b> — монопольное использование<br  />
<code>#PBS -r n</code> — является ли задача перезапускаемой (задачей с контрольными точками);
<b>y</b> — является, <b>n</b> — не является<br  />
<code>#PBS -M user@mail.com</code> — почтовый адрес пользователя<br  />
<code>#PBS -m abe</code> — какие сообщения отправляются на указанный адрес (<b>a</b> — ошибка в
выполнении задачи, <b>b</b> — начало выполнения, <b>e</b> — завершение
выполнения)<br  />
<code>#PBS -q workq</code> — идентификатор очереди<br  />
<code>#PBS -N mpi_test</code> — название задачи<br  />
<code>#!/bin/sh</code> — указание необходимого командного интерпретатора<br  />
<code>cd /home/user/test/mpi_test</code> — переход в директорию с исполняемым файлом<br  />
<code>module unload spectrum_mpi &amp;&amp; module load openmpi/gcc/1.10.6/4.8.5</code> — выбор openmpi вместо spectrum_mpi<br  />
<code>mpirun ./mpi</code> — запуск приложения<br  />
<code>exit 0</code> — выход
</p>
</div>
</div>

<div id="outline-container-orgheadline13" class="outline-4">
<h4 id="interactive"><a id="orgheadline13"></a><span class="section-number-4">4.4.4</span> Запуск интерактивных программ</h4>
<div class="outline-text-4" id="text-interactive">
<p>
Программы, использующие стандартный ввод, называются
интерактивными. Как правило, такие программы после запуска требуют от
пользователя ввода данных. При постановке задачи в очередь любая
программа переводится в фоновый режим. В этом режиме ввод данных
пользователем в запущенную программу невозможен. Для передачи данных
таким программам используется механизм перенаправления стандартных
потоков ввода/вывода.
</p>

<p>
Для перенаправления подготавливается текстовый файл, содержимое
которого в точности представляет собой данные, вводимые пользователем.
Например, если программа <code>solver</code> предполагает ввод в первой строке
размерности матрицы, а во второй количества итераций, то текстовый
файл <code>input.txt</code> будет иметь вид:
</p>

<div class="org-src-container">

<pre class="src src-raw">[user@jupiter solver]$ cat input.txt
10000000
1000
</pre>
</div>

<p>
После каждого числа обязателен символ новой строки. Запуск программы
на выполнение производится так:
</p>

<div class="org-src-container">

<pre class="src src-raw">solver &lt; input.txt
</pre>
</div>

<p>
Скрипт для постановки в очередь задания, в рамках которого будет
выполняться интерактивная программа, будет выглядеть следующим
образом:
</p>

<div class="org-src-container">

<pre class="src src-raw">[user@jupiter solver]$ cat job.qsub
#PBS -k oe
#PBS -l select=1:ncpus=8:mpiprocs=8
#PBS -r n
#PBS -M user@mail.com
#PBS -m abe
#PBS -q workq
#PBS -N solver
#!/bin/sh

cd /home/user/test/solver

mpirun ./solver &lt; ./input.txt

exit 0
</pre>
</div>
</div>
</div>

<div id="outline-container-orgheadline14" class="outline-4">
<h4 id="nonparallel"><a id="orgheadline14"></a><span class="section-number-4">4.4.5</span> Запуск непараллельных программ</h4>
<div class="outline-text-4" id="text-nonparallel">
<p>
Запуск непараллельных программ практически ничем не отличается от запуска параллельных
программ. Единственное отличие заключается в том, что в <code>qsub</code> скрипте такой программы необходимо
указать, что для её работы необходим только один логический процессор:
</p>

<div class="org-src-container">

<pre class="src src-raw">#PBS -l select=1:ncpus=1
</pre>
</div>

<p>
Также в этом скрипте необходимо запускать непосредственно исполняемый
файл программы, то есть не использовать для запуска <code>mpiexec</code>.
</p>
</div>
</div>

<div id="outline-container-orgheadline15" class="outline-4">
<h4 id="user_tasks"><a id="orgheadline15"></a><span class="section-number-4">4.4.6</span> Состояние пользовательских задач</h4>
<div class="outline-text-4" id="text-user_tasks">
<p>
Для получения информации об очередях и задачах пользователя
используется команда <code>qstat</code>. Выполнение этой команды без параметров
покажет все задачи пользователя и их состояние.
</p>

<div class="org-src-container">

<pre class="src src-raw">[user@jupiter ~]$ qstat
Job id                  Name             User            Time Use S Queue
----------------------- ---------------- --------------- -------- - -----
700.jupiter1            mpi_test         user            00:10:40 R workq
701.jupiter1            sample_job       user            0        Q workq
702.jupiter1            solver           user            0        Q workq
</pre>
</div>

<p>
<b>Job id</b> — идентификатор задачи, полученный при выполнении <code>qsub</code>;
<b>Name</b> — имя задачи; <b>User</b> — имя пользователя, запустившего задачу;
<b>Time Use</b> — процессорное время, потраченное задачей; <b>S (State)</b> — с
остояние задачи ( <b>R</b> – задача выполняется, <b>Q</b> – ожидает в очереди);
<b>Queue</b> — очередь.
</p>

<p>
В данном случае пользователю <b>user</b> принадлежат три задачи.
</p>

<p>
С помощью команды <code>qstat -n идентификатор_задачи</code> можно получить
список узлов, на которых выполняется конкретная задача. Эта информация
полезна при мониторинге эффективности использования вычислительных
ресурсов с использованием системы Ganglia, так как позволяет
отслеживать состояние только используемых задачей узлов.
</p>

<div class="org-src-container">

<pre class="src src-raw">[user@jupiter ~]$ qstat -n 701

jupiter1:
                                                           Req'd  Req'd   Elap
Job ID          Username Queue    Jobname    SessID NDS TSK Memory Time  S Time
--------------- -------- -------- ---------- ------ --- --- ------ ----- - -----
298.jupiter1    eab      workq    mpi_test      --    1   4    2gb 00:00 R   -- 
   jupiter1/0*4
</pre>
</div>

<p>
Для получения более подробной информации о конкретной задаче можно запустить команду
<code>qstat -f идентификатор_задачи</code>.
</p>
</div>
</div>

<div id="outline-container-orgheadline16" class="outline-4">
<h4 id="stop_tasks"><a id="orgheadline16"></a><span class="section-number-4">4.4.7</span> Остановка задач</h4>
<div class="outline-text-4" id="text-stop_tasks">
<p>
Остановка программы производится командой <code>qdel идентификатор_задачи</code>
</p>
<div class="org-src-container">

<pre class="src src-raw">[user@jupiter ~]$ qdel 700
</pre>
</div>

<p>
Этой командой задача, стоящая в очереди, убирается из нее, а
выполняющаяся задача снимается с выполнения. Следующая по очереди и
приоритету задача встает на выполнение.
</p>

<p>
Задача снимается в течении некоторого времени, поэтому при вызове
<code>qstat</code> непосредственно после <code>qdel</code> удаленная задача все еще может
быть отражена в таблице.
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-orgheadline21" class="outline-2">
<h2 id="monitoring"><a id="orgheadline21"></a><span class="section-number-2">5</span> Мониторинг</h2>
<div class="outline-text-2" id="text-monitoring">
</div>

<div id="outline-container-orgheadline19" class="outline-3">
<h3 id="web_interface"><a id="orgheadline19"></a><span class="section-number-3">5.1</span> Web-интерфейс</h3>
<div class="outline-text-3" id="text-web_interface">
<p>
Мониторинг кластера реализован при помощи системы Ganglia. Эта система позволяет следить
за ресурсами кластера посредством web-интерфейса. Система мониторинга находится по адресу 
<a href="http://jupiter.febras.net/ganglia">http://jupiter.febras.net/ganglia</a>.
</p>

<p>
Для мониторинга пользователю доступно большое число типов ресурсов:
загруженность процессора, оперативная память, загрузка сети, средняя
загрузка, количество процессов и ряд других.  Имеется возможность
наблюдать как за всеми узлами в кластере (по одному параметру), так и
за каждым (по всем параметрам).
</p>
</div>
</div>


<div id="outline-container-orgheadline20" class="outline-3">
<h3 id="console"><a id="orgheadline20"></a><span class="section-number-3">5.2</span> Консоль</h3>
<div class="outline-text-3" id="text-console">
<p>
Кроме графического интерфейса существует несколько полезных консольных
команд для мониторинга. Команда <code>pbsnodes имя_узла</code> позволяет
получить информацию о конкретном узле: тип, состояние, количество
процессоров, выполняющиеся задачи. Ниже представлен фрагмент вывода
этой команды.
</p>

<div class="org-src-container">

<pre class="src src-raw">[user@jupiter ~]$ pbsnodes jupiter2
jupiter2
     Mom = jupiter2
     Port = 15002
     pbs_version = 14.1.0
     ntype = PBS
     state = free
     pcpus = 160
     resources_available.arch = linux
     resources_available.host = jupiter2
     resources_available.mem = 263653568kb
     resources_available.ncpus = 160
     resources_available.ngpus = 2
     resources_available.vnode = jupiter2
     resources_assigned.accelerator_memory = 0kb
     resources_assigned.mem = 0kb
     resources_assigned.naccelerators = 0
     resources_assigned.ncpus = 0
     resources_assigned.netwins = 0
     resources_assigned.ngpus = 0
     resources_assigned.vmem = 0kb
     resv_enable = True
     sharing = default_shared
</pre>
</div>

<p>
<b>state</b> – состояние узла (<b>job-exclusive</b> – все ресурсы узла заняты;
<b>free</b> – на узле есть свободные ресурсы для запуска заданий; <b>offline</b>
– узел временно выведен из эксплуатации, запуск заданий на нем
невозможен; <b>down</b> – узел выключен); <b>pcpus</b> – число процессорных
потоков на узле; <b>jobs</b> – задачи, запущенные на узле;
<b>resources_available.ngpus</b> – число GPU на узле.
</p>

<p>
При выполнении команды <code>pbsnodes -a -S -j</code> будет выведена сводная информация обо
всех узлах кластера.
</p>

<div class="org-src-container">

<pre class="src src-raw">[user@jupiter ~]$ pbsnodes -a -S -j
                                                        mem       ncpus   nmics   ngpus
vnode           state           njobs   run   susp      f/t        f/t     f/t     f/t   jobs
--------------- --------------- ------ ----- ------ ------------ ------- ------- ------- -------
jupiter1        free                 0     0      0  251gb/251gb 160/160     0/0     2/2 --
jupiter2        free                 0     0      0  251gb/251gb 160/160     0/0     2/2 --
jupiter3        free                 0     0      0  251gb/251gb 160/160     0/0     2/2 --
jupiter4        free                 0     0      0  251gb/251gb 160/160     0/0     2/2 --
jupiter5        free                 0     0      0  251gb/251gb 160/160     0/0     2/2 --
</pre>
</div>
</div>
</div>
</div>

<div id="outline-container-orgheadline22" class="outline-2">
<h2 id="reference"><a id="orgheadline22"></a><span class="section-number-2">6</span> Справочная информация</h2>
<div class="outline-text-2" id="text-reference">
<p>
Описание основных команд при работе в ОС Linux – <a href="http://wwwinfo.jinr.ru/unixinfo/pc/lin_os.html">http://wwwinfo.jinr.ru/unixinfo/pc/lin_os.html</a>
Документация к системе диспетчеризации заданий PBS Pro — <a href="http://www.pbsworks.com/pdfs/PBSUserGuide14.2.pdf">http://www.pbsworks.com/pdfs/PBSUserGuide14.2.pdf</a>
</p>
</div>
</div>
</div>
</body>
</html>
